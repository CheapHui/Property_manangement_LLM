import os
import json
import requests
from typing import List
from pathlib import Path
from pypdf import PdfReader
from typing import Optional
import subprocess
from docx import Document
import re
from dotenv import load_dotenv

# Load environment variables from .env file
ROOT = Path(__file__).resolve().parent.parent
load_dotenv(ROOT.parent / ".env")

# ========= API Configuration (loaded from .env) =========
DEEPSEEK_API_KEY = os.getenv("OPENAI_API_KEY")  # DeepSeek API key from .env
BASE_URL = os.getenv("OPENAI_API_BASE", "https://api.deepseek.com")
MODEL_NAME = "deepseek-chat"  # or "deepseek-reasoner"
# =================================
CONTROL_CHARS_RE = re.compile(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]")

def antiword_extract(doc_path: str, timeout_sec: int = 30) -> str:
    doc_abs = str(Path(doc_path).resolve())
    cmd = ["antiword", doc_abs]
    proc = subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=timeout_sec)
    return proc.stdout or ""

def lo_cat_text(doc_path: str, timeout_sec: int = 90) -> str:
    soffice = "/Applications/LibreOffice.app/Contents/MacOS/soffice"
    doc_abs = str(Path(doc_path).resolve())

    cmd = [
        soffice,
        "--headless",
        "--nologo",
        "--nodefault",
        "--norestore",
        "--nolockcheck",
        "--cat",   # âœ… ç›´æ¥ dump text
        doc_abs
    ]

    print("ğŸ› ï¸ LO --cat:", Path(doc_abs).name)
    proc = subprocess.run(
        cmd,
        check=True,
        capture_output=True,
        text=True,
        timeout=timeout_sec
    )
    return proc.stdout or ""

def docx_to_text(docx_path: str) -> str:
    doc = Document(docx_path)
    parts = []

    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t:
            parts.append(t)

    # tables
    for table in doc.tables:
        for row in table.rows:
            row_text = []
            for cell in row.cells:
                ct = (cell.text or "").strip().replace("\n", " ")
                if ct:
                    row_text.append(ct)
            if row_text:
                parts.append(" | ".join(row_text))

    return "\n\n".join(parts)


def doc_to_txt(
    doc_path: str,
    txt_path: str,
    encoding: str = "utf-8",
    temp_convert_dir: str = "./_tmp_doc_convert",
    timeout_sec: int = 60   # âœ… åŠ å‘¢è¡Œ
) -> str:
    """
    ä¸»è·¯ç·šï¼š
    - .doc  : LibreOffice -> .txtï¼ˆæœ‰ timeoutï¼Œå””æœƒå¡æ­»ï¼‰
    - .docx : python-docx æŠ½ text
    """
    doc_path_p = Path(doc_path)
    ext = doc_path_p.suffix.lower()

    if ext == ".doc":
        out_txt = convert_doc_to_txt_via_lo(
            str(doc_path_p),
            temp_convert_dir,
            timeout_sec=timeout_sec   # âœ… å‚³è½å»
        )
        with open(out_txt, "r", encoding=encoding, errors="ignore") as f:
            full_text = f.read()

    elif ext == ".docx":
        full_text = docx_to_text(str(doc_path_p))

    else:
        raise ValueError(f"Unsupported file type: {ext}")

    txt_path_p = Path(txt_path)
    txt_path_p.parent.mkdir(parents=True, exist_ok=True)
    with open(txt_path_p, "w", encoding=encoding) as f:
        f.write(full_text)

    print(f"âœ“ Saved TXT: {txt_path_p}")
    return str(txt_path_p)

def convert_doc_to_txt_via_lo(doc_path: str, out_dir: str, timeout_sec: int = 60) -> str:
    """
    ä¸»è·¯ç·šï¼šLibreOffice convert-to txt (UTF-8)
    fallbackï¼š
      1) --cat dump textï¼ˆè¼ƒç©©ï¼‰
      2) antiwordï¼ˆç¬¬ä¸‰å±¤ï¼‰
    """
    os.makedirs(out_dir, exist_ok=True)

    soffice = "/Applications/LibreOffice.app/Contents/MacOS/soffice"

    profile_dir = Path(out_dir) / "_lo_profile"
    profile_dir.mkdir(parents=True, exist_ok=True)
    user_install = profile_dir.resolve().as_uri()

    doc_abs = str(Path(doc_path).resolve())
    out_abs = str(Path(out_dir).resolve())

    cmd = [
        soffice,
        "--headless",
        "--nologo",
        "--nodefault",
        "--norestore",
        "--nolockcheck",
        f"-env:UserInstallation={user_install}",
        "--convert-to", "txt:Text (encoded):UTF8",
        "--outdir", out_abs,
        doc_abs,
    ]

    base = Path(doc_abs).stem
    out_txt = Path(out_abs) / f"{base}.txt"

    print("ğŸ› ï¸ LO convert:", Path(doc_abs).name, "-> txt")

    def _return_if_exists() -> Optional[str]:
        if out_txt.exists() and out_txt.stat().st_size > 0:
            return str(out_txt)
        candidates = list(Path(out_abs).glob(f"{base}*.txt"))
        if candidates and candidates[0].stat().st_size > 0:
            return str(candidates[0])
        return None

    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=timeout_sec)
        got = _return_if_exists()
        if got:
            return got
        raise FileNotFoundError(f"Converted txt not found for {doc_abs}")

    except (subprocess.TimeoutExpired, subprocess.CalledProcessError) as e:
        # å…ˆè©¦ --catï¼ˆé€šå¸¸æ•‘åˆ°ä¸€å¤§æ‰¹ï¼‰
        if isinstance(e, subprocess.TimeoutExpired):
            print(f"â³ convert-to timeout ({timeout_sec}s). Fallback to --cat: {Path(doc_abs).name}")
        else:
            print(f"âš ï¸ convert-to failed (exit={e.returncode}). Fallback to --cat: {Path(doc_abs).name}")

        try:
            text = lo_cat_text(doc_abs, timeout_sec=120).strip()
            if text:
                with open(out_txt, "w", encoding="utf-8") as f:
                    f.write(text)
                return str(out_txt)
        except Exception:
            pass

        # å†è©¦ antiwordï¼ˆç¬¬ä¸‰å±¤ï¼‰
        print(f"ğŸ§¯ Fallback to antiword: {Path(doc_abs).name}")
        try:
            text = antiword_extract(doc_abs, timeout_sec=30).strip()
            if text:
                with open(out_txt, "w", encoding="utf-8") as f:
                    f.write(text)
                return str(out_txt)
        except Exception:
            pass

        # æœ€å¾Œå¯« error logï¼ˆå¦‚æœä¿‚ CalledProcessError æ‰æœ‰ stdout/stderrï¼‰
        log_path = Path(out_abs) / f"{base}.lo_error.txt"
        with open(log_path, "w", encoding="utf-8") as f:
            f.write("CMD:\n" + " ".join(cmd) + "\n\n")
            if isinstance(e, subprocess.CalledProcessError):
                f.write("STDOUT:\n" + ((e.stdout or "").strip()) + "\n\n")
                f.write("STDERR:\n" + ((e.stderr or "").strip()) + "\n")
            else:
                f.write(f"ERROR:\nTimeout after {timeout_sec}s\n")

        raise RuntimeError(f"LibreOffice convert failed. See: {log_path}") from e
    
def is_case_done(jsonl_path: Path, min_lines: int = 1) -> bool:
    """
    åˆ¤æ–· case æ˜¯å¦å·²å®Œæˆï¼š
    - æª”æ¡ˆå­˜åœ¨
    - æª”æ¡ˆ size > 0
    - è‡³å°‘æœ‰ min_lines è¡Œï¼ˆæ¯è¡Œä¸€å€‹ chunkï¼‰
    """
    if not jsonl_path.exists():
        return False
    if jsonl_path.stat().st_size <= 0:
        return False
    try:
        with jsonl_path.open("r", encoding="utf-8") as f:
            cnt = 0
            for _ in f:
                cnt += 1
                if cnt >= min_lines:
                    return True
        return False
    except Exception:
        return False


def mark_case_failed(jsonl_path: Path, reason: str):
    """
    å¤±æ•—æ™‚å°‡è¼¸å‡ºæª”æ”¹ååš .failedï¼Œé¿å…ä¸‹æ¬¡èª¤åˆ¤ç‚º doneã€‚
    """
    try:
        failed_path = jsonl_path.with_suffix(jsonl_path.suffix + ".failed")
        if jsonl_path.exists():
            jsonl_path.rename(failed_path)
        # åŒæ™‚å¯«åŸå› 
        with open(str(failed_path) + ".reason.txt", "w", encoding="utf-8") as f:
            f.write(reason)
    except Exception:
        pass

def sanitize_for_json(s: str) -> str:
    """
    ç§»é™¤ JSON string ä¸å…è¨±çš„æ§åˆ¶å­—å…ƒ
    (ä¿ç•™ \n \r \t)
    """
    if not s:
        return s
    # å…ˆçµ±ä¸€æ›è¡Œ
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # ç§»é™¤å…¶é¤˜æ§åˆ¶å­—å…ƒ
    s = CONTROL_CHARS_RE.sub("", s)
    return s

def docx_to_text(docx_path: str) -> str:
    doc = Document(docx_path)
    parts = []

    # paragraphs
    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t:
            parts.append(t)

    # tables (table æ–‡å­—éƒ½å¥½é‡è¦)
    for table in doc.tables:
        for row in table.rows:
            row_text = []
            for cell in row.cells:
                ct = (cell.text or "").strip()
                ct = ct.replace("\n", " ").strip()
                row_text.append(ct)
            line = " | ".join([x for x in row_text if x])
            if line:
                parts.append(line)

    return "\n\n".join(parts)


def convert_doc_to_docx(doc_path: str, out_dir: str) -> str:
    """
    ç”¨ LibreOffice å°‡ .doc è½‰ .docx
    - éœ€è¦å·²å®‰è£ LibreOfficeï¼Œä¸¦ä¸” soffice åœ¨ PATH å…§ï¼ˆæˆ–ä½ æ”¹æˆå®Œæ•´è·¯å¾‘ï¼‰
    """
    os.makedirs(out_dir, exist_ok=True)

    # Windows ä¾‹å­ï¼šå¦‚æœä½  PATH ç„¡ sofficeï¼Œå¯æ”¹æˆï¼š
    # soffice = r"C:\Program Files\LibreOffice\program\soffice.exe"
    soffice = "soffice"

    cmd = [
        soffice,
        "--headless",
        "--convert-to", "docx",
        "--outdir", out_dir,
        doc_path
    ]
    subprocess.run(cmd, check=True,timeout=TimeoutError)

    base = Path(doc_path).stem
    converted = str(Path(out_dir) / f"{base}.docx")
    if not os.path.exists(converted):
        raise FileNotFoundError(f"LibreOffice convert failed, docx not found: {converted}")
    return converted



def pdf_to_txt(
    pdf_path: str,
    txt_path: Optional[str] = None,
    skip_first_pages: int = 0,
    max_pages: Optional[int] = None,
    encoding: str = "utf-8"
) -> str:
    """
    å°‡å–®ä¸€ PDF è½‰åš TXTã€‚

    :param pdf_path: PDF æª”è·¯å¾‘
    :param txt_path: è¼¸å‡º TXT æª”è·¯å¾‘ï¼Œå¦‚ç‚º None å‰‡ç”¨åŒå .txt
    :param skip_first_pages: è·³éé ­å¹¾é ï¼ˆä¾‹å¦‚ index / å°é¢ï¼‰
    :param max_pages: æœ€å¤šè™•ç†å¹¾å¤šé ï¼ˆNone = å…¨éƒ¨ï¼‰
    :param encoding: è¼¸å‡º TXT ç·¨ç¢¼
    :return: è¼¸å‡º TXT æª”å¯¦éš›è·¯å¾‘
    """
    pdf_path = Path(pdf_path)

    reader = PdfReader(str(pdf_path))

    num_pages = len(reader.pages)
    start_page = min(skip_first_pages, num_pages)
    if max_pages is not None:
        end_page = min(start_page + max_pages, num_pages)
    else:
        end_page = num_pages

    print(f"Reading PDF: {pdf_path}")
    print(f"Total pages: {num_pages}, processing pages: {start_page} to {end_page - 1}")

    texts = []
    for i in range(start_page, end_page):
        page = reader.pages[i]
        try:
            page_text = page.extract_text() or ""
        except Exception as e:
            print(f"âš ï¸ Page {i} extract_text() error: {e}")
            page_text = ""
        texts.append(page_text)

    full_text = "\n\n==== PAGE BREAK ====\n\n".join(texts)
    
    # Save to file if txt_path is provided
    if txt_path:
        txt_path = Path(txt_path)
        txt_path.parent.mkdir(parents=True, exist_ok=True)
        with open(txt_path, "w", encoding=encoding) as f:
            f.write(full_text)
        print(f"âœ“ Saved TXT: {txt_path}")
        return str(txt_path)
    
    return full_text

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€åç²¾é€šé¦™æ¸¯ç‰©æ¥­ç®¡ç†åŠæ³•å¾‹æ–‡ä»¶è™•ç†çš„æ³•å¾‹åŠ©ç†ï¼Œè² è²¬æŠŠé¦™æ¸¯æ³•é™¢åˆ¤è©è½‰æ›æˆé©åˆ RAG ä½¿ç”¨çš„ JSON çµæ§‹ã€‚

è¼¸å‡ºè¦æ±‚ï¼š
- åªè¼¸å‡º JSONï¼ˆä¸åŒ…å«ä»»ä½•è§£é‡‹ã€å‰æ–‡å¾Œç†ã€æ¨™é¡Œæˆ–è¨»è§£ï¼‰
- JSON çš„ key åå¿…é ˆèˆ‡æˆ‘æä¾›çš„ schema å®Œå…¨ä¸€è‡´
- å¦‚æŸäº›æ¬„ä½ç„¡æ³•å¾æ–‡æœ¬åˆ¤æ–·ï¼Œè«‹è¨­ç‚º null æˆ–ç©ºé™£åˆ— []ï¼Œä¸è¦äº‚çŒœæ¡ˆä»¶å…§å®¹
- ç”¨é¦™æ¸¯ä¸­æ–‡åŠç¹é«”å­—å›ç­”
- ä½ å¿…éœ€åªå¯ä»¥é¸ç”¨æˆ‘æä¾›çš„ property_topic, ordinance_sections, building_type, roles_involved, outcome é¸é …
- è¼¸å‡ºæ ¼å¼ï¼š
- MUST è¼¸å‡ºä¸€å€‹ JSON object
- JSON object æ ¼å¼å¿…é ˆä¿‚ï¼š
  {
    "chunks": [ ...chunk objects... ]
  }
- ä¸è¦è¼¸å‡ºå…¶ä»–ä»»ä½•æ–‡å­—
- è¼¸å‡º JSON æ™‚ï¼Œè«‹ç§»é™¤æ‰€æœ‰ä¸å¯è¦‹æ§åˆ¶å­—å…ƒï¼ˆé™¤ \n, \t å¤–ï¼‰ã€‚
- æ¯æ¬¡æœ€å¤šè¼¸å‡º 4 å€‹ chunk objectsã€‚è‹¥æ–‡æœ¬å¤ªé•·ï¼Œåªè¼¸å‡ºæœ€é‡è¦çš„ 4 å€‹ã€‚

----------------------------------------------------------------
### METADATA VOCABULARY (STRICT)
----------------------------------------------------------------

## property_topic (use ONLY values below; may choose 1â€“3 items)
OC_powers
OC_litigation_rights
OC_meeting_procedures
OC_resolution_validity
OC_election_and_officers
management_fee
special_levy
sinking_fund
budget_and_accounts
building_maintenance
common_parts_dispute
water_seepage
unauthorized_structure
fire_safety
nuisance
use_restrictions
access_dispute
parking_and_vehicles
DMC_interpretation
manager_role_under_DMC
management_company_liability
service_contracts
BMO_interpretation
land_lease_and_title
government_enforcement
land_tribunal_procedure

## roles_involved (choose all roles that appear)
Owners_Corporation
Individual_Owner
Joint_Owners
Management_Company
Manager_under_DMC
Tenant
Developer
Contractor
Government_Department
Incorporated_Owners_Other_Building

## ordinance_sections (ONLY values matching this format):
<ShortName>_Cap<Number>_s<Section>
Examples:
BMO_Cap344_s16
BMO_Cap344_s18
BMO_Cap344_s34I
BO_Cap123_s24
FSO_Cap95_s9
LTO_Cap219_s41

## building_type (choose 0â€“1)
residential
composite
industrial
commercial
shopping_centre
village_house
mixed_use_estate

## outcome (choose 1)
Plaintiff_succeeds
Defendant_succeeds
Mixed
Dismissed
Settlement
Appeal_allowed
Appeal_dismissed
null

## section_type (choose 1 per chunk)
headnote
facts
issues
reasoning
order
procedural_history

ä½ è¦è™•ç†çš„æ˜¯ã€Œå–®ä¸€å®— court caseã€ï¼Œè¼¸å…¥æœƒåŒ…å«ï¼š
- åŸºæœ¬è³‡æ–™ï¼ˆcourt, case no., parties, date ç­‰ï¼Œå¦‚æœ‰ï¼‰
- åˆ¤è©å…§æ–‡ï¼ˆå¯èƒ½åªä¿‚å…¶ä¸­ä¸€éƒ¨åˆ†ï¼‰

ä½ éœ€è¦ï¼š
1. åˆ†ææ¡ˆä»¶å¤§æ„ï¼ˆèˆ‡é¦™æ¸¯ç‰©æ¥­ç®¡ç†ç›¸é—œçš„é‡é»ï¼Œä¾‹å¦‚ï¼šç®¡ç†è²»ã€ç¶­ä¿®è²»ã€DMC è§£é‡‹ã€OC æ¬ŠåŠ›ã€ç®¡ç†å…¬å¸è²¬ä»»ç­‰ï¼‰
2. æŠŠåˆ¤è©åˆ‡æˆå¤šå€‹ content chunkï¼Œæ¯å€‹ chunk ä»£è¡¨ä¸€æ®µé‡è¦ reasoning / è§£é‡‹æ–‡æœ¬
3. ç‚ºæ¯å€‹ chunk è¼¸å‡ºä¸€è¡Œ JSONï¼ˆlist å½¢å¼ï¼‰ï¼Œç”¨ä»¥ä¸‹ schemaï¼š

æ¯ä¸€å€‹ chunk object æ‡‰è©²æœ‰ä»¥ä¸‹æ¬„ä½ï¼š

{
  "id": "case_<CASE_ID>_chunk1",
  "case_id": "HKCFI_2020_123",
  "source": "HKCFI",
  "case_no": "HCMP 123/2020",
  "court": "Court of First Instance",
  "judgment_date": "2020-05-12",
  "year": 2020,
  "parties": "Owners' Corporation of XXX Building v YYY Management Co Ltd",
  "url": null,

  "property_topic": [], // Only from vocabulary above
  "ordinance_sections": [], // ONLY from vocab or empty
  "building_type": [], // ONLY from vocab or empty
  "roles_involved": [], ONLY from vocab
  "outcome": null, //MUST use vocab
  "is_leading_case": false,

  "paragraph_range": "[12]-[18]",
  "content": "é€™å€‹ chunk çš„åŸæ–‡å…§å®¹...",

  "content_summary": "ç”¨ 2-4 å¥ä¸­æ–‡ç¸½çµé€™æ®µåœ¨ç‰©æ¥­ç®¡ç†ä¸Šçš„é‡é»ã€‚",
  "issue_summary": "ç”¨ 1-2 å¥ä¸­æ–‡æ¦‚æ‹¬é€™æ®µæ¶‰åŠçš„æ³•å¾‹çˆ­é»ã€‚",

  "category": "court_case",
  "section_type": "reasoning",

  "embedding_model": null
}

è¼¸å‡ºæ ¼å¼ï¼š
- MUST è¼¸å‡ºä¸€å€‹ JSON arrayï¼ˆlistï¼‰ï¼Œæ¯å€‹å…ƒç´ ä¿‚ä¸€å€‹ chunk object
- ä¸è¦è¼¸å‡ºå…¶ä»–ä»»ä½•æ–‡å­—
"""

# ===============================
# 1. å°‡å¥½é•·å˜…åˆ¤è© TEXT å…ˆåˆ‡æˆå¹¾ä»½ sub-text
# ===============================

def split_text_into_chunks(text: str,
                           max_chars: int = 8000,
                           overlap_chars: int = 500) -> List[str]:
    """
    ç”¨æ®µè½ (\n\n) ç‚ºå–®ä½ï¼Œå°‡é•·æ–‡æœ¬åˆ‡æˆå¤šå€‹ chunkã€‚
    - max_chars: æ¯å€‹ chunk ç›®æ¨™æœ€å¤§å­—å…ƒæ•¸ï¼ˆç²—ç•¥è¿‘ä¼¼ token limitï¼‰
    - overlap_chars: chunk èˆ‡ chunk ä¹‹é–“ä¿ç•™å¤šå°‘å­—å…ƒ overlapï¼ˆç”¨æœ€å¾Œå¹¾å€‹æ®µè½ï¼‰
    """
    paragraphs = text.split("\n\n")
    chunks = []
    current = []
    current_len = 0

    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
        p_len = len(p) + 2  # åŠ ç•ª \n\n

        if current_len + p_len > max_chars and current:
            # å®Œæˆä¸€å€‹ chunk
            chunk_text = "\n\n".join(current)
            chunks.append(chunk_text)

            # æº–å‚™ä¸‹ä¸€å€‹ chunkï¼šåŠ  overlap
            if overlap_chars > 0:
                # ç”±å°¾é–‹å§‹æ€è¿”å•² paragraph åš overlap
                overlap = []
                overlap_len = 0
                for para in reversed(current):
                    l = len(para) + 2
                    if overlap_len + l > overlap_chars:
                        break
                    overlap.insert(0, para)
                    overlap_len += l
                current = overlap + [p]
                current_len = sum(len(x) + 2 for x in current)
            else:
                current = [p]
                current_len = p_len
        else:
            current.append(p)
            current_len += p_len

    if current:
        chunks.append("\n\n".join(current))

    return chunks


# ===============================
# 2. DeepSeek APIï¼šè™•ç†ä¸€å€‹ sub-text â†’ JSON chunks
# ===============================

def call_deepseek_court_case_to_chunks(case_text: str, retries: int = 2) -> List[dict]:
    """
    å‘¼å« DeepSeekï¼ŒæŠŠä¸€æ®µ court case æ–‡æœ¬è½‰åš chunk JSON listã€‚
    - æœ‰ retryï¼šç•¶æ¨¡å‹è¼¸å‡ºå””ä¿‚åˆæ³• JSON array æ™‚æœƒé‡è©¦
    """
    url = f"{BASE_URL}/v1/chat/completions"

    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }

    case_text = sanitize_for_json(case_text)

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": case_text}
        ],
        "stream": False,
        "temperature": 0.1,
        "max_tokens": 8192,
        "response_format": {"type": "json_object"}
    }

    last_err = None

    

    for attempt in range(retries + 1):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=120)
            resp.raise_for_status()
            data = resp.json()

            content = data["choices"][0]["message"]["content"].strip()

            # åªæ“·å– JSON arrayï¼ˆç”±ç¬¬ä¸€å€‹ [ åˆ°æœ€å¾Œä¸€å€‹ ]ï¼‰
            start = content.find("[")
            end = content.rfind("]")
            if start != -1 and end != -1 and end > start:
                content_json = content[start:end + 1]
            else:
                content_json = content  # fallback

            content_json = sanitize_for_json(content_json)
            chunks = json.loads(content_json)

            if not isinstance(chunks, list):
                raise ValueError("Model output is not a JSON array (list).")
            

            return chunks



        except Exception as e:
            last_err = e
            # dump raw outputï¼ˆå¦‚æœæœ‰ï¼‰
            try:
                with open(f"deepseek_bad_output_attempt{attempt}.txt", "w", encoding="utf-8") as f:
                    f.write(content if "content" in locals() else str(e))
            except Exception:
                pass

            if attempt == retries:
                raise

            print(f"âš ï¸ DeepSeek output parse failed, retrying... attempt {attempt+1}/{retries}")

    # ç†è«–ä¸Šå””æœƒåˆ°å‘¢åº¦
    raise last_err


# ===============================
# 3. å°‡ã€Œå¥½å¤š sub-textã€å…¨éƒ¨ä¸Ÿå» DeepSeek å†åˆä½µ
# ===============================

def process_long_case_text_to_chunks(case_text: str,
                                     case_id: str,
                                     max_chars: int = 5000,
                                     overlap_chars: int = 200) -> List[dict]:
    """
    - å…ˆå°‡è¶…é•·åˆ¤è©åˆ‡æˆå¤šå€‹ sub-text chunk
    - é€å€‹ sub-text ä¸Ÿå» DeepSeek åš JSON chunks
    - æœ€å¾Œåˆä½µæ™’ï¼Œä¸¦çµ±ä¸€é‡æ’ idï¼š<case_id>_chunk1/2/3/...
    """
    # 1) åˆ‡ text
    text_chunks = split_text_into_chunks(
        case_text,
        max_chars=max_chars,
        overlap_chars=overlap_chars
    )

    print(f"åˆ†æ‹†ç‚º {len(text_chunks)} å€‹ sub-text chunk")

    all_chunks: List[dict] = []

    for idx, sub_text in enumerate(text_chunks, start=1):
        print(f"--> è™•ç† sub-text {idx}/{len(text_chunks)}")
        sub_chunks = call_deepseek_court_case_to_chunks(sub_text)
        all_chunks.extend(sub_chunks)

    # 2) çµ±ä¸€é‡è¨­ idï¼ŒåŒæ™‚ç¢ºä¿ case_id ä¸€è‡´
    for i, ch in enumerate(all_chunks, start=1):
        # å¦‚æœ model å†‡ç”¨ä½ æä¾›å˜… case_idï¼Œå°±ä»¥ä½ å‚³å…¥å˜…ç‚ºæº–
        ch["case_id"] = case_id
        ch["id"] = f"{case_id}_chunk{i}"

    return all_chunks


# ===============================
# 4. å¯«å…¥ JSONL æª”
# ===============================

def write_chunks_to_jsonl(chunks: List[dict], output_path: str):
    """
    å°‡ chunks list å¯«å…¥ JSONL æª”ï¼Œæ¯è¡Œä¸€å€‹ JSON objectã€‚
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        for row in chunks:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")


# ===============================
# 5. mainï¼šç¤ºç¯„è™•ç†å–®ä¸€å®— case
# ===============================

def main():
    input_pdf_folder = "./Count_case"
    intermediate_txt_folder = "../data/raw/cases_txt"
    output_jsonl_folder = "./data/chunked/court_cases"

    pdf_folder = Path(input_pdf_folder)
    txt_folder = Path(intermediate_txt_folder)
    jsonl_folder = Path(output_jsonl_folder)

    txt_folder.mkdir(parents=True, exist_ok=True)
    jsonl_folder.mkdir(parents=True, exist_ok=True)

    # æ‰¾æ‰€æœ‰ DOC / DOCX
    doc_files = sorted(
        f for f in pdf_folder.iterdir()
        if f.is_file()
        and f.suffix.lower() in [".doc", ".docx"]
        and not f.name.startswith("~$")
    )
    print(f"ğŸ“ åœ¨è³‡æ–™å¤¾æ‰¾åˆ° {len(doc_files)} å€‹ DOC/DOCX æª”æ¡ˆ")

    for doc_file in doc_files:
        case_id = doc_file.stem
        txt_output_path = txt_folder / f"{case_id}.txt"
        jsonl_output_path = jsonl_folder / f"{case_id}.jsonl"

        # å·²å®Œæˆå°± skip
        if is_case_done(jsonl_output_path, min_lines=1):
            print(f"â­ï¸ Skip (already ingested): {case_id}")
            continue

        # å¦‚ä¹‹å‰å¤±æ•—éï¼šä½ å¯é¸æ“‡ skip / retry
        failed_marker = jsonl_output_path.with_suffix(".jsonl.failed")
        if failed_marker.exists():
            print(f"âš ï¸ Found failed marker, re-trying: {case_id}")

        print("\n=============================================")
        print(f"ğŸ“„ é–‹å§‹è™•ç†ï¼š{doc_file.name}")
        print("=============================================")

        try:
            # 1) DOC/DOCX â†’ TXTï¼ˆä¸»è·¯ç·šï¼šdoc è½‰ txtï¼‰
            print("â†’ Step 1: DOC/DOCX âœ TXT")
            doc_to_txt(
                doc_path=str(doc_file),
                txt_path=str(txt_output_path),
                temp_convert_dir=str(txt_folder / "_tmp_doc_convert"),
                timeout_sec=60
            )

            # 2) è®€å…¥ TXT
            print("â†’ Step 2: è®€å– TXT")
            with open(txt_output_path, "r", encoding="utf-8") as f:
                case_text = f.read()

            # 3) AI chunking
            print("â†’ Step 3: DeepSeek AI chunking")
            chunks = process_long_case_text_to_chunks(
                case_text=case_text,
                case_id=case_id,
                max_chars=8000,
                overlap_chars=500
            )

            # 4) å¯«å…¥ JSONLï¼šå…ˆå¯« tmp å† replaceï¼Œé¿å…åŠæˆå“
            print(f"â†’ Step 4: å¯«å…¥ JSONLï¼š{jsonl_output_path}")
            tmp_path = jsonl_output_path.with_suffix(".jsonl.tmp")
            write_chunks_to_jsonl(chunks, str(tmp_path))
            os.replace(tmp_path, jsonl_output_path)

            print(f"âœ… å®Œæˆ {case_id}ï¼ˆå…± {len(chunks)} å€‹ chunksï¼‰")

        except Exception as e:
            msg = str(e)

            if "timed out" in msg and "--cat" in msg:
                reason = "LO_CAT_TIMEOUT"
            elif "Unterminated string" in msg:
                reason = "DEEPSEEK_JSON_TRUNCATED"
            elif "Invalid control character" in msg:
                reason = "DEEPSEEK_CONTROL_CHARS"
            else:
                reason = "OTHER"

            print(f"âŒ Case failed: {case_id} â€” {reason} â€” {e}")
            mark_case_failed(jsonl_output_path, f"{reason}\n{e}")
            continue

    print("\nğŸ‰ å…¨éƒ¨ DOC/DOCX å·²è™•ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
